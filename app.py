import os
import asyncio
import time
import threading
import nest_asyncio
from dotenv import load_dotenv

import streamlit as st
from pydantic import BaseModel
from typing import Optional, Dict, Any, List
from io import BytesIO

import fitz  # PyMuPDF for PDF extraction
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.lib.pagesizes import LETTER

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import StructuredTool
from langchain_core.documents import Document
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

from langchain_community.tools.tavily_search import TavilySearchResults
from langgraph.graph import StateGraph, END

from gpt_researcher import GPTResearcher
import gpt_researcher.actions.agent_creator as agent_creator

# === Allow nested loops for Streamlit ===
nest_asyncio.apply()
load_dotenv()
os.environ["REPORT_SOURCE"] = "local"

# === Streamlit page ===
st.set_page_config(page_title="Retail Hyderabad Researcher", page_icon="üîç", layout="centered")
st.markdown("""
    <style>
    body {
        background-color: #0D0D0D;
        color: white;
    }
    .stApp {
        background-color: #0D0D0D;
    }
    div[data-testid="stHeader"] {
        background: #0D0D0D;
    }
    input {
        background-color: #1A1A1A !important;
        color: white !important;
        font-size: 1.2rem !important;
    }
    </style>
""", unsafe_allow_html=True)

# === GPTResearcher fix ===
original = agent_creator.extract_json_with_regex
def safe_extract_json_with_regex(response):
    if not response:
        return None
    return original(response)
agent_creator.extract_json_with_regex = safe_extract_json_with_regex

# === Core LLM ===
llm = ChatOpenAI(model="gpt-4.1-nano", temperature=0.2)
embeddings = OpenAIEmbeddings()
tavily = TavilySearchResults(k=1)
INDEX_PATH = "./faiss_index"

async def get_hyderabad_fact() -> str:
    """Get a live Hyderabad retail fact."""
    q = "Give me one recent interesting fact about retail in Hyderabad or Inorbit Mall Hyderabad."
    try:
        res = tavily.invoke({"query": q})
        if res and "results" in res and res["results"]:
            return res["results"][0]["content"].strip()
    except Exception:
        pass
    return "No recent Hyderabad retail fact found."

# === Load or build vector ===
if os.path.exists(INDEX_PATH):
    vs = FAISS.load_local(INDEX_PATH, embeddings, allow_dangerous_deserialization=True)
else:
    vs = None

if vs is None:
    st.markdown("<h1 style='text-align: center; font-size: 3rem;'>Retail Hyderabad Researcher</h1>", unsafe_allow_html=True)
    uploaded_file = st.file_uploader("üìÇ Upload your PDF to index", type=["pdf"])
    if uploaded_file:
        with st.spinner("Indexing PDF..."):
            pdf_bytes = uploaded_file.read()
            pdf_doc = fitz.open(stream=pdf_bytes, filetype="pdf")
            pages = [page.get_text().strip() for page in pdf_doc if page.get_text().strip()]
            docs = [Document(page_content=t) for t in pages]
            if docs:
                vs = FAISS.from_documents(docs, embeddings)
                vs.save_local(INDEX_PATH)
                st.success("‚úÖ Vector store created! Please reload.")
            else:
                st.warning("No text found. Try another PDF.")
    st.stop()

# === RAG chains ===
def get_retriever_chain():
    retriever = vs.as_retriever(search_type="similarity", search_kwargs={"k": 8})
    prompt = ChatPromptTemplate.from_messages([
        MessagesPlaceholder("chat_history"),
        ("user", "{input}"),
        ("user", "Generate a precise search query.")
    ])
    return create_history_aware_retriever(llm, retriever, prompt)

def get_rag_chain(chain):
    prompt = ChatPromptTemplate.from_messages([
        ("system", """
You are a retail research assistant.
Use ONLY this context. If none, reply ‚ùå.
Context: {context}
        """),
        MessagesPlaceholder("chat_history"),
        ("user", "{input}"),
    ])
    return create_retrieval_chain(chain, create_stuff_documents_chain(llm, prompt))

async def vector_lookup(query: str) -> str:
    """Vector store lookup with Hyderabad fact."""
    docs = vs.similarity_search(query, k=5)
    if not docs:
        return "‚ùå No vector match."
    context = "\n\n".join([d.page_content for d in docs])
    chain = get_rag_chain(get_retriever_chain())
    result = await chain.ainvoke({"chat_history": [], "input": query, "context": context})
    fact = await get_hyderabad_fact()
    return f"{result['answer']}\n\nüí° **Hyderabad Retail Fact:** {fact}"

async def chitchat_tool(query: str) -> str:
    """Casual reply with Hyderabad fact."""
    prompt = f'User said: "{query}". Reply politely in 1‚Äì2 lines.'
    resp = (await llm.ainvoke(prompt)).content.strip()
    fact = await get_hyderabad_fact()
    return f"{resp}\n\nüí° **Hyderabad Retail Fact:** {fact}"

# === Researcher ===
class CustomLogsHandler:
    def __init__(self):
        self.logs: List[Dict[str, Any]] = []

    async def send_json(self, data: Dict[str, Any]) -> None:
        self.logs.append(data)

async def run_gpt_researcher(query: str, logs_handler: CustomLogsHandler) -> str:
    """Run deep GPTResearcher with Hyderabad fact."""
    researcher = GPTResearcher(
        query=query,
        report_type="research_report",
        report_source="hybrid",
        vector_store=vs,
        websocket=logs_handler
    )
    await researcher.conduct_research()
    report = await researcher.write_report()
    fact = await get_hyderabad_fact()
    return f"{report}\n\nüí° **Hyderabad Retail Fact:** {fact}"

def run_gpt_researcher_sync(query: str, logs_handler: CustomLogsHandler) -> str:
    return asyncio.get_event_loop().run_until_complete(run_gpt_researcher(query, logs_handler))

# === LangGraph ===
vector_tool = StructuredTool.from_function(vector_lookup)
chitchat = StructuredTool.from_function(chitchat_tool)

class State(BaseModel):
    query: str
    route: Optional[str] = None
    answer: Optional[str] = None

async def router(state: State):
    res = await llm.ainvoke(f'Classify this: "{state.query}". Return vector OR chitchat.')
    return {"route": res.content.strip().lower()}

async def vector_node(state: State):
    return {"answer": await vector_tool.ainvoke({"query": state.query})}

async def chitchat_node(state: State):
    return {"answer": await chitchat.ainvoke({"query": state.query})}

graph = StateGraph(State)
graph.add_node("router", router)
graph.add_node("vector", vector_node)
graph.add_node("chitchat", chitchat_node)
graph.set_entry_point("router")
graph.add_conditional_edges("router", lambda s: s.route, {"vector": "vector", "chitchat": "chitchat"})
graph.add_edge("vector", END)
graph.add_edge("chitchat", END)
agent = graph.compile()

# === Main UI ===
async def main():
    st.markdown(
        "<h1 style='text-align: center; font-size: 4rem;'>perplexity</h1>",
        unsafe_allow_html=True
    )

    if "messages" not in st.session_state:
        st.session_state.messages = []

    if "deep_query" not in st.session_state:
        st.session_state["deep_query"] = ""

    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    query = st.text_input(
        "",
        placeholder="Ask anything or @mention a Space",
        label_visibility="collapsed"
    )

    if query:
        st.session_state.messages.append({"role": "user", "content": query})
        with st.chat_message("user"):
            st.markdown(query)

        with st.spinner("Thinking..."):
            result = await agent.ainvoke(State(query=query))
            answer = result["answer"]
            if asyncio.iscoroutine(answer):
                answer = await answer

            st.session_state.messages.append({"role": "assistant", "content": answer})

            if answer.startswith("‚ùå"):
                if st.button("üöÄ Run Deep Research Now"):
                    st.session_state["deep_query"] = query
                    st.experimental_rerun()

        st.rerun()

    st.subheader("Deep Research")
    deep_q = st.text_input("Topic for Deep Research:", key="deep_query")
    if st.button("üöÄ Run Deep Research"):
        if not deep_q.strip():
            st.warning("Please enter a topic.")
        else:
            def stream_research():
                logs_handler = CustomLogsHandler()
                result_holder = {"report": ""}

                def run_and_store():
                    result_holder["report"] = run_gpt_researcher_sync(deep_q, logs_handler)

                t = threading.Thread(target=run_and_store)
                t.start()

                last_index = 0
                while t.is_alive():
                    time.sleep(1)
                    new_logs = logs_handler.logs[last_index:]
                    for log in new_logs:
                        yield f"üîç **{log.get('content','')}**\n\n{log.get('output','')}\n\n---"
                    last_index += len(new_logs)

                final_report = result_holder["report"]
                yield f"\n\n## ‚úÖ Final Report\n\n{final_report}"

                pdf_buffer = BytesIO()
                doc = SimpleDocTemplate(pdf_buffer, pagesize=LETTER)
                styles = getSampleStyleSheet()
                story = []
                for line in final_report.split("\n"):
                    if line.strip().startswith("# "):
                        story.append(Paragraph(f"<b>{line.strip('# ').strip()}</b>", styles["Heading1"]))
                    elif line.strip().startswith("## "):
                        story.append(Paragraph(f"<b>{line.strip('# ').strip()}</b>", styles["Heading2"]))
                    elif line.strip().startswith("### "):
                        story.append(Paragraph(f"<b>{line.strip('# ').strip()}</b>", styles["Heading3"]))
                    elif line.strip() == "---":
                        story.append(Spacer(1, 12))
                    else:
                        story.append(Paragraph(line, styles["BodyText"]))
                    story.append(Spacer(1, 6))

                doc.build(story)
                pdf_buffer.seek(0)
                st.download_button(
                    "üìÑ Download Report as PDF",
                    data=pdf_buffer,
                    file_name="deep_research.pdf",
                    mime="application/pdf"
                )

            st.write_stream(stream_research)

asyncio.run(main())
